#!/usr/bin/env python3
"""
Clean German Dataset Splits
- Reads the raw split TSV files (train.tsv, validation.tsv, test.tsv).
- Normalizes the 'sentence' column by removing leading/trailing punctuation and whitespace.
- Saves the cleaned data to new files with a '_cleaned' suffix.

Usage:
    python3 clean_german_splits.py --tsv_dir /home/abhinav.pm/ABHI/SAL/v2/130_hours_exps/german_dataset_130_tsv_splits
"""

import pandas as pd
import os
import argparse
from tqdm import tqdm

def normalize_sentence(sentence):
    """
    Cleans a sentence by removing leading/trailing punctuation and whitespace.
    """
    if not isinstance(sentence, str):
        return sentence
    # Characters to strip from the beginning and end of sentences
    chars_to_strip = '"â€â€žÂ«Â»â€˜â€™`â€”â€“- \t'
    normalized_sentence = sentence.strip().strip(chars_to_strip).strip()
    return normalized_sentence

def clean_tsv_file(input_path, output_path):
    """
    Reads a TSV file, cleans the 'sentence' column, and saves it to a new file.
    """
    if not os.path.exists(input_path):
        print(f"  -âš ï¸  Warning: File not found, skipping: {input_path}")
        return 0, 0
        
    print(f"  - Processing: {os.path.basename(input_path)}")
    
    # Read TSV
    df = pd.read_csv(input_path, sep='\t')
    
    # Keep track of original to count changes
    df['original_sentence'] = df['sentence']
    
    # Apply normalization
    tqdm.pandas(desc="    Normalizing sentences")
    df['sentence'] = df['sentence'].progress_apply(normalize_sentence)
    
    # Count changes
    changes = (df['sentence'] != df['original_sentence']).sum()
    
    # Remove the temp column
    df = df.drop(columns=['original_sentence'])
    
    # Save to new file
    df.to_csv(output_path, sep='\t', index=False)
    
    print(f"    âœ“ Cleaned file saved to: {os.path.basename(output_path)}")
    print(f"    - Sentences changed: {changes:,} / {len(df):,}")
    
    return len(df), changes

def main():
    parser = argparse.ArgumentParser(description="Clean German split TSV data files.")
    parser.add_argument(
        "--tsv_dir",
        type=str,
        # This matches the OUTPUT_DIR from your splitting script
        default="/home/abhinav.pm/ABHI/SAL/v2/german_dataset_tsv_splits", 
        help="Directory containing the split TSV files."
    )
    args = parser.parse_args()
    
    print("="*70)
    print(" " * 18 + "German Dataset Cleaning Script")
    print("="*70)
    print(f"ðŸ” Looking for TSV files in: {args.tsv_dir}\n")
    
    # The files generated by the splitting script are named simply:
    files_to_clean = {
        "train": "train_130.tsv",
        "validation": "validation_5.tsv",
        "test": "test_5.tsv"
    }
    
    total_samples, total_changes = 0, 0
    for split, filename in files_to_clean.items():
        input_file = os.path.join(args.tsv_dir, filename)
        
        # Create output filename: train.tsv -> train_cleaned.tsv
        base, ext = os.path.splitext(filename)
        output_file = os.path.join(args.tsv_dir, f"{base}_cleaned{ext}")
        
        samples, changes = clean_tsv_file(input_file, output_file)
        total_samples += samples
        total_changes += changes
        print("-" * 50)

    print("\n" + "="*70)
    print("ðŸŽ‰ Cleaning Complete!")
    print("="*70)
    print(f"  - Total samples processed: {total_samples:,}")
    print(f"  - Total sentences modified: {total_changes:,}")
    print("\nNext Steps:")
    print("1. Use these '_cleaned.tsv' files for Phonemization.")
    print("="*70)

if __name__ == "__main__":
    main()