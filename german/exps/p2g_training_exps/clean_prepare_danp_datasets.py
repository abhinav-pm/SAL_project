#!/usr/bin/env python3
"""
Prepare Final DANP Datasets
- Combines beam search and random sampling data.
- De-duplicates the combined data to create the final training set.
- Also saves de-duplicated versions of the individual files for ablation studies.

This script corresponds to the data preparation needed for the experiments in
Table 2 of the paper, such as '32-beam + sampling'.

Usage:
    # Combine beam and sampling, then de-duplicate
    python f_prepare_danp_datasets.py \
        --beam_tsv ./p2g_training_data/train_danp_beam32.tsv \
        --sampling_tsv ./p2g_training_data/train_danp_sampling500.tsv \
        --output_dir ./p2g_final_datasets

    # De-duplicate only a single file
    python f_prepare_danp_datasets.py \
        --beam_tsv ./p2g_training_data/train_danp_beam32.tsv \
        --output_dir ./p2g_final_datasets
"""

import pandas as pd
import os
import argparse
from tqdm import tqdm

def process_dataset(output_dir, output_filename, *dfs):
    """
    Combines one or more dataframes, de-duplicates, shuffles, and saves.
    """
    print("\n" + "="*70)
    print(f"Processing for: {output_filename}")
    print("="*70)

    # 1. Concatenate all provided dataframes
    if not dfs:
        print("  - No dataframes provided. Skipping.")
        return

    print("  - [1/4] Combining datasets...")
    combined_df = pd.concat(dfs, ignore_index=True)
    print(f"    ‚úì Total rows before de-duplication: {len(combined_df):,}")

    # 2. De-duplicate based on the 'phonemes' column
    # We keep the 'first' entry, which is not critical since the sentence is the same.
    print("  - [2/4] De-duplicating based on 'phonemes' column...")
    
    # Check if 'phonemes' column exists
    if 'phonemes' not in combined_df.columns:
        print("  ‚ùå ERROR: 'phonemes' column not found. Cannot de-duplicate.")
        return

    dedup_df = combined_df.drop_duplicates(subset=['phonemes'], keep='first')
    print(f"    ‚úì Total rows after de-duplication: {len(dedup_df):,}")
    
    num_removed = len(combined_df) - len(dedup_df)
    print(f"    - Removed {num_removed:,} duplicate phoneme sequences.")

    # 3. Shuffle the dataset
    # This is good practice to ensure batches during training are well-mixed.
    print("  - [3/4] Shuffling the final dataset...")
    shuffled_df = dedup_df.sample(frac=1, random_state=42).reset_index(drop=True)
    print("    ‚úì Dataset shuffled.")

    # 4. Save the final dataset
    print("  - [4/4] Saving the processed file...")
    output_path = os.path.join(output_dir, output_filename)
    
    # Select essential columns to save space
    columns_to_save = ['path', 'sentence', 'phonemes']
    final_df = shuffled_df[columns_to_save]
    
    final_df.to_csv(output_path, sep='\t', index=False)
    
    size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"    ‚úì Saved {len(final_df):,} rows to: {output_filename} ({size_mb:.1f} MB)")
    print("="*70)


def main():
    parser = argparse.ArgumentParser(
        description="Combine and de-duplicate DANP datasets.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        "--beam_tsv",
        type=str,
        help="Path to the DANP TSV generated by beam search."
    )
    parser.add_argument(
        "--sampling_tsv",
        type=str,
        help="Path to the DANP TSV generated by random sampling."
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="Directory to save the final de-duplicated and combined datasets."
    )
    
    args = parser.parse_args()

    # --- Validation ---
    if not args.beam_tsv and not args.sampling_tsv:
        print("‚ùå ERROR: You must provide at least one input file (--beam_tsv or --sampling_tsv).")
        return

    if args.beam_tsv and not os.path.exists(args.beam_tsv):
        print(f"‚ùå ERROR: Beam search file not found at: {args.beam_tsv}")
        return

    if args.sampling_tsv and not os.path.exists(args.sampling_tsv):
        print(f"‚ùå ERROR: Random sampling file not found at: {args.sampling_tsv}")
        return

    os.makedirs(args.output_dir, exist_ok=True)

    print("="*70)
    print(" "*15 + "DANP DATASET PREPARATION")
    print("="*70)

    # --- Load Data ---
    df_beam = None
    df_sampling = None

    if args.beam_tsv:
        print(f"[*] Loading beam search data from: {args.beam_tsv}")
        df_beam = pd.read_csv(args.beam_tsv, sep='\t')
        print(f"    ‚úì Loaded {len(df_beam):,} rows.")
        # Create a de-duplicated version of just the beam data
        beam_filename = os.path.basename(args.beam_tsv).replace('.tsv', '_dedup.tsv')
        process_dataset(args.output_dir, beam_filename, df_beam)

    if args.sampling_tsv:
        print(f"[*] Loading random sampling data from: {args.sampling_tsv}")
        df_sampling = pd.read_csv(args.sampling_tsv, sep='\t')
        print(f"    ‚úì Loaded {len(df_sampling):,} rows.")
        # Create a de-duplicated version of just the sampling data
        sampling_filename = os.path.basename(args.sampling_tsv).replace('.tsv', '_dedup.tsv')
        process_dataset(args.output_dir, sampling_filename, df_sampling)

    # --- Combine and Process ---
    if df_beam is not None and df_sampling is not None:
        # Create a combined, de-duplicated dataset
        combined_filename = "train_danp_beam_plus_sampling_dedup.tsv"
        process_dataset(args.output_dir, combined_filename, df_beam, df_sampling)

    print("\nüéâ All processing complete!")
    print(f"Final datasets are saved in: {args.output_dir}")

if __name__ == "__main__":
    main()